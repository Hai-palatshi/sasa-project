# -*- coding: utf-8 -*-
"""Classification_Of_COVID_19_v17.0.1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10QPuUXFtFRySLtg5rtLw80fS7TlBDlIS

# Installs and Imports

## Installs
"""

# is_debug = True
is_debug = False
is_debug_2 = False
is_debug_3 = False

!pip install dtaidistance
!pip install scikit-learn-extra
!pip install tensorflow
!pip install keras
!pip install gensim
!pip install scipy
!pip install biopython
!pip install pydot
!pip install graphviz
!pip install nltk
!pip install tqdm

"""## Imports"""

import sys
import os
import csv
import matplotlib.pyplot as plt
import numpy as np
from numpy import random
import gc
import string
import glob
import pandas as pd
from google.colab import drive
from dtaidistance import dtw
import collections
from Bio import SeqIO
from tqdm import tqdm
from pathlib import Path
from datetime import datetime
import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
nltk.download("stopwords")
from nltk.tokenize import word_tokenize
import pickle

from sklearn.ensemble import IsolationForest, RandomForestRegressor, RandomForestClassifier
from sklearn.cluster import KMeans
from sklearn_extra.cluster import KMedoids
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import accuracy_score, f1_score

import tensorflow as tf
from tensorflow.keras.layers import (
    MaxPooling1D,
    Conv1D,
    Input,
    Flatten,
    Dense,
    Bidirectional,
    Dropout,
    LSTM,
    Layer,
    Embedding,
    BatchNormalization,
    Masking
)
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import plot_model, to_categorical
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, Callback
from tensorflow.keras.regularizers import l2

import warnings
warnings.filterwarnings(action = 'ignore')

import gensim.downloader as api
from gensim.models import KeyedVectors, Word2Vec
from gensim.utils import simple_preprocess
from typing import Union, Tuple, Dict, Optional

random_seed = 42

"""# Parametrs"""

# t_model = None
# word_vectors = None
# params_obj = {
#             'file_path': "",
#             'imposter_a': "",
#             'imposter_b': "",
#             'embedding_algorithm': "",
#             'dimensional': "",
#             'chunk': ""
#         }
# iteration_num = 1

# sum_dic_summ = {}
# sum_dicForest = {}
# sum_labels = {}
# colab_path = ""
# colab_output_path = ""

iteration_num = 1
data_base_dict = {}
moving_average_window_size = 10
is_train_word2vec = False

"""# Connection To Drive"""

drive.mount('/content/drive')

colab_path = "/content/drive/My Drive/Classification Of COVID-19 Using Imposters Method/data_base/"
colab_output_path = "/content/drive/My Drive/Classification Of COVID-19 Using Imposters Method/output/"
colab_word2vec_model_path = "/content/drive/My Drive/Classification Of COVID-19 Using Imposters Method/word2vec_model/"
sys.path.append(os.path.abspath(colab_path))
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
data_file_path = f"{colab_output_path}data_{timestamp}.pkl"
imposters_pairs_file_path = f"{colab_output_path}imposters_pairs_{timestamp}.pkl"
df_csv_path = f"{colab_output_path}results_{timestamp}.csv"
model_path = f"{colab_word2vec_model_path}word2vec.model"
checkpoint_path = f"{colab_output_path}checkpoints"

"""# Main

הפונקציה הראשית שמבצעת את תהליך הסיווג והחישוב עבור הטקסטים. היא מנהלת את כל התהליך
מאתחול הנתונים, הטענת המידע, הגדרת פרמטרים, קריאה לפונקציות השונות לצורך עיבוד והכנת דוחות, וכל זאת במספר איטרציות.
כל איטרציה מייצגת שילוב בין שני אימפוסטרים עבור השוואת הטקסטים
"""

# Main function to classify text from the Shakespeare dataset using imposters data, and to save the classification results, distance matrices, and relevant reports. This function processes data in multiple iterations, each involving a pair of imposters, and writes the results into CSV and NPY files.
def main(is_train_word2vec, data_file_path, iteration_num, data_base_dict, moving_average_window_size, df_csv_path):
  dtw_simliarity_list = []
  if not is_debug_2:
    print("Getting covid19 data")
    get_folder_data("covid19")
    print("Getting imposters data")
    get_folder_data("imposters")
    print("Preparing data for Word2Vec")
    sentences = prepare_data_for_word2vec()
    if is_train_word2vec:
      print("Training Word2Vec model")
      model = create_model_for_gen2vec(sentences)
    else:
      print("Loading Word2Vec model")
      model = Word2Vec.load(model_path)
    print("Getting Word2Vec matrices")
    add_word2vec_matrices(model)
    imposters_pairs = create_imposters_pairs(data_base_dict['imposters'])
    cnn_model = None
    for imposters_pair in tqdm(imposters_pairs, desc="Training model on imposters pairs"): # 21
      # Set classification parameters
      p = {
        'file_path': colab_path + "text to classify.txt",
        'imposter_a': imposters_pair['imposter_a'],
        'imposter_b': imposters_pair['imposter_b'],
        'moving_average_window_size': moving_average_window_size,
        'dimensional': "300",
        'chunk': "400"
      }
      print(f"len(imposters_pair['imposter_a']['X']): {len(imposters_pair['imposter_a']['X'])}")
      print(f"len(imposters_pair['imposter_b']['X']): {len(imposters_pair['imposter_b']['X'])}")
      print("===========================================================")
      print('Iteration = ', iteration_num)
      print('Imposter A = ', p['imposter_a']['name'])
      print('Imposter B = ', p['imposter_b']['name'])
      print("===========================================================")

      # Perform text classification with the given parameters
      cnn_model, imposters_pair['y_pred'], imposters_pair['covid_variant_dtw_simliarity_list'] = classify_text(p, cnn_model)
      for covid_variant_dtw_simliarity in imposters_pair['covid_variant_dtw_simliarity_list']:
        imposters_pair_dict = {}
        imposters_pair_dict['variant_type'] = 'covid'
        imposters_pair_dict['dtw_similarity_score'] = covid_variant_dtw_simliarity['dtw_similarity_score']
        dtw_simliarity_list.append(imposters_pair_dict)
      iteration_num += 1
    # Save the trained model
    cnn_model.save(f"{colab_output_path}model{timestamp}.keras")
    print("Finish CNN model")


    imposters_pair_pairs_dict_list = []
    for i in range(len(imposters_pairs)):
      for j in range(i + 1, len(imposters_pairs)):
        imposters_pair_pairs_dict = {}
        imposters_pair_pairs_dict['imposters_pair_a'] = imposters_pairs[i]
        imposters_pair_pairs_dict['imposters_pair_b'] = imposters_pairs[j]
        imposters_pair_pairs_dict_list.append(imposters_pair_pairs_dict)
        imposters_pair_a_imposter_a_name = imposters_pair_pairs_dict['imposters_pair_a']['imposter_a']['name']
        imposters_pair_a_imposter_b_name = imposters_pair_pairs_dict['imposters_pair_a']['imposter_b']['name']
        imposters_pair_b_imposter_a_name = imposters_pair_pairs_dict['imposters_pair_b']['imposter_a']['name']
        imposters_pair_b_imposter_b_name = imposters_pair_pairs_dict['imposters_pair_b']['imposter_b']['name']
        imposters_name = f"imposters_pair_a_imposter_a_name: {imposters_pair_a_imposter_a_name}, imposters_pair_a_imposter_b_name: {imposters_pair_a_imposter_b_name},\
        imposters_pair_b_imposter_a_name: {imposters_pair_b_imposter_a_name}, imposters_pair_b_imposter_b_name: {imposters_pair_b_imposter_b_name}"
        print(imposters_name)

    print(f"len(imposters_pairs): {len(imposters_pairs)}")
    print(f"Number of imposters pair pairs: {len(imposters_pair_pairs_dict_list)}")  # 210

    for imposters_pair_pairs_dict in tqdm(imposters_pair_pairs_dict_list, desc="Calculate dtw simliraity between imposters pair pairs"):
        imposters_pair_a = imposters_pair_pairs_dict['imposters_pair_a']
        imposters_pair_b = imposters_pair_pairs_dict['imposters_pair_b']
        print(f"imposters_pair_a['y_pred']: {imposters_pair_a['y_pred']}")
        imposters_pair_a_moving_average = calculate_moving_averages(imposters_pair_a['y_pred'], moving_average_window_size)
        imposters_pair_b_moving_average = calculate_moving_averages(imposters_pair_b['y_pred'], moving_average_window_size)
        imposters_pair_dict = {}
        imposters_pair_dict['variant_type'] = 'imposters_pairs_pair'
        imposters_pair_dict['dtw_similarity_score'] = calculate_dtw_simliraity(imposters_pair_a_moving_average, imposters_pair_b_moving_average)
        dtw_simliarity_list.append(imposters_pair_dict)

    print(f"len(dtw_simliarity_list): {len(dtw_simliarity_list)}")
    sorted_dtw_simliarity_list =  sorted(dtw_simliarity_list, key=lambda d: d['dtw_similarity_score'], reverse=True)
    print(f"len(sorted_dtw_simliarity_list): {len(sorted_dtw_simliarity_list)}")
    sorted_dtw_similarity_score_df = pd.DataFrame(sorted_dtw_simliarity_list)
    print(f"sorted_dtw_similarity_score_df['variant_type'].value_counts(): {sorted_dtw_similarity_score_df['variant_type'].value_counts()}")

    print(f"df['variant_type'][:10]: {sorted_dtw_similarity_score_df['variant_type'][:10]}, df['dtw_similarity_score'][:10]: {sorted_dtw_similarity_score_df['dtw_similarity_score'][:10]}")
    data_dict = {'imposters_pairs': imposters_pairs,
                'imposters_pair_pairs_dict_list': imposters_pair_pairs_dict_list,
                'sorted_dtw_similarity_score_df': sorted_dtw_similarity_score_df}
    with open(data_file_path, 'wb') as f:
      pickle.dump(data_dict, f)
    sorted_dtw_similarity_score_df.to_csv(df_csv_path)
  else:
    data_file_path = f"{colab_output_path}data_20241109_204213.pkl"
    with open(data_file_path, 'rb') as f:
      data_dict = pickle.load(f)
      sorted_dtw_similarity_score_df = data_dict['sorted_dtw_similarity_score_df']
      print(f"sorted_dtw_similarity_score_df: {sorted_dtw_similarity_score_df}")
  # X = sorted_dtw_similarity_score_df.copy()
  # y = X.pop('variant_type')

  # value_counts = y.value_counts()
  # print(f"value_counts:\n {value_counts}")

  # X['is_outlier_isolation_forest'] = detect_outliers(X)
  # df = pd.concat([X, y], ignore_index=True)
  # df_with_outliers_isolation_forest = pd.concat([X['dtw_similarity_score'], y, X['is_outlier_isolation_forest']], axis=1)

  # analysis_isolation_forest = analyze_outliers(X['dtw_similarity_score'], X['is_outlier_isolation_forest'])
  # print(analysis_isolation_forest)

  X = sorted_dtw_similarity_score_df.copy()

  y = X.pop('variant_type')
  y_true = binary_encode(y)
  print(f"X: {X}")
  X['is_outlier_random_forest'] = detect_outliers_rf(X)
  df_with_outliers_random_forest = pd.concat([X['dtw_similarity_score'], y_true, X['is_outlier_random_forest']], axis=1)
  print(df_with_outliers_random_forest)
  analysis_random_forest = analyze_rf_outliers(df_with_outliers_random_forest['dtw_similarity_score'], df_with_outliers_random_forest['is_outlier_random_forest'])
  print(analysis_random_forest)
  is_outlier_detection = True
  y_pred = binary_encode(df_with_outliers_random_forest['is_outlier_random_forest'], is_outlier_detection)
  rf_accuracy_score = accuracy_score(y_true, y_pred)
  rf_f1_score = f1_score(y_true, y_pred)
  print(f"Random Forest, accuracy_score: {rf_accuracy_score}")
  print(f"Random Forest, f1_score: {rf_f1_score}")
  data_dict['df_with_outliers_random_forest'] = df_with_outliers_random_forest
  # perform_kmedoids_clustering(X)

  perform_kmeans_clustering(X)
  return data_dict

"""## Sub Functions And Classes

create_data_list()

The function create_data_list performs the following tasks:

Takes a list of file paths: It accepts a list of file paths (file_list) as a parameter.

Initializes an empty list: The function creates an empty list called data, where the content of each file will be stored.

Iterates through the file list:

The function loops over each file in the list (file_list).
For each file, it opens the file in read mode (open) with utf-8 encoding. If there are any encoding errors, they are ignored using errors='ignore'.
The content of the file is read entirely with f_input.read() and then replaces every newline character (\n) with a space using .replace('\n', ' ').
The processed content is added to the data list.
Returns the list: After the loop, the function returns the data list, which contains the text content of each file.

Error handling: If an error occurs while reading any file, the error message is printed, and the function returns an empty list.
"""

# Reads a list of text files and returns their contents
# Parameters:
  # file_path_list - list, A list of file paths pointing to text files to be read and processed.
# Returns:
  # file_dict_list - list, A list of file dictionaries.

def create_data_list(file_path_list):
  # Initialize an empty list to store the file contents
  file_dict_list = []

  # Loop through each file path in the input file list
  for file_path in file_path_list:
    # Open the file, reading with UTF-8 encoding and ignoring errors
    file_dict = {}
    file_dict['file_path'] = file_path
    print(f"file_path: {file_path}")
    file_dict['name'] = Path(file_path).stem
    with open(file_path, "r", encoding = 'utf-8', errors = 'ignore') as f_input:
      file_dict['file_content'] = f_input.read().replace('\n', '').strip()
      file_dict_list.append(file_dict)
  # Return the list of file contents
  return file_dict_list

""" get_covid19_data()

 This function retrieves the covid19 data files and processes them into a list that stores in the global dictionary `data_base_dict` under the key 'covid19'.
"""

# Create a list of files that represent covid19 viruses
def get_folder_data(folder_name):
  # Get the list of all .fasta files in the covid19 directory
  path = colab_path + folder_name
  print(f"path: {path}")
  file_list = glob.glob(os.path.join(path, "*.fasta")) + glob.glob(os.path.join(path, "*.fa"))

  print(f"file_list: {file_list}")
  # Read and process the content of each file using create_data_list
  data_list = create_data_list(file_list)

  # Store the processed data in the global dictionary under the key 'covid19'
  data_base_dict[folder_name] = data_list

if is_debug:
  # Load the covid19 data
  get_folder_data("covid19")

if is_debug:
  # Load the covid19 data
  get_folder_data("imposters")

"""split_text_to_words()"""

# Splits the input text into chunks of the specified size.
# Parameters:
  # text - str, The input string to be split.
  # word_length - int, The number of characters each word should contain. defult is 4
# Returns:
  # list, A list of strings, where each string is a chunk of the original text with a length of 'word_size'.
def split_text_to_words(text, word_length = 4):
  return [text[i:i + word_length] for i in range(0, len(text), word_length)]

"""combine_words_into_sentence()"""

# Groups a list of words into sentences with a specified number of words.
# Parameters:
  # text - list, A list of words to be grouped.
  # sentence_length - int, The number of words each sentence should contain. defult is 10
# Returns:
  # list, A list of sentences, where each sentence contains 'sentence_length' words.
def combine_words_into_sentence(words, sentence_length = 16):
  return [words[i:i + sentence_length] for i in range(0, len(words), sentence_length)]

"""create_model_for_gen2vec()"""

def prepare_data_for_word2vec():
  folder_names = ['covid19', 'imposters']
  sentences = []
  for folder_name in folder_names:
    folder_data = data_base_dict[folder_name]
    for file_dict in folder_data:
        words = split_text_to_words(file_dict['file_content'])
        file_dict_sentences = combine_words_into_sentence(words)
        sentences.extend(file_dict_sentences)
  return sentences

if is_debug:
  sentences = prepare_data_for_word2vec()

# Creates a Word2Vec model by combining dataset of imposters and covid19 text data, processes them into words and sentences, and then trains a Word2Vec model to create word embeddings.
# Returns:
  # gensim.models.Word2Vec, A trained Word2Vec model with word embeddings for the combined dataset.
def create_model_for_gen2vec(sentences):
  # Train a Word2Vec model
  # sg: Training algorithm — 1 for skip-gram.
  model = Word2Vec(sentences = sentences, vector_size = 300, window = 5, min_count = 0, sg = 1)
  model.train(sentences, total_examples = len(sentences), epochs = 1)
  # Save the model
  model.save(model_path)
  return model

if is_debug:
  model = create_model_for_gen2vec(sentences)

def get_word2vec_matrix(model, file_dict, is_shuffle_word2vec):
    words = split_text_to_words(file_dict['file_content'])
    file_dict_sentences = combine_words_into_sentence(words)
    word2vecs = []
    vector_size = model.wv.vector_size
    for sentence in file_dict_sentences:
        sentence_vector = np.zeros(vector_size)
        valid_words = 0
        for word in sentence:
            try:
                # Add the word vector to the sentence vector
                sentence_vector += model.wv[word]
                valid_words += 1
            except KeyError:
                # Skip words not in vocabulary
                continue
        if valid_words > 0:
            sentence_vector = sentence_vector / valid_words
        word2vecs.append(sentence_vector)
    np_word2vecs = np.array(word2vecs)
    if is_shuffle_word2vec and is_debug_3:
      file_dict['X'] = random.permutation(np_word2vecs)
    else:
      file_dict['X'] = np_word2vecs
    return file_dict

def add_word2vec_matrices(model):
  folder_names = ['covid19', 'imposters']
  for folder_name in tqdm(folder_names):
    folder_data = data_base_dict[folder_name]
    for file_dict_index, file_dict in tqdm(enumerate(folder_data)):
        if folder_name == 'imposters' and file_dict_index == 1:
          is_shuffle_word2vec = True
          get_word2vec_matrix(model, file_dict, is_shuffle_word2vec)
        else:
          is_shuffle_word2vec = False
          get_word2vec_matrix(model, file_dict, is_shuffle_word2vec)

if is_debug:
  add_word2vec_matrices()

def create_imposters_pairs(imposters_data):
  # Create a list of all possible unique imposter pairs
  imposters_pairs = []
  if is_debug_3:
    imposters_dict = {}
    imposters_dict['imposter_a'] = imposters_data[0]
    imposters_dict['imposter_b'] = imposters_data[1]
    imposters_pairs.append(imposters_dict)
  else:
    for i in range(len(imposters_data)):
      for j in range(i + 1, len(imposters_data)):
        imposters_dict = {}
        imposters_dict['imposter_a'] = imposters_data[i]
        imposters_dict['imposter_b'] = imposters_data[j]
        imposters_pairs.append(imposters_dict)
  return imposters_pairs

if is_debug:
  # get imposters pairs
  imposters_pairs = create_imposters_pairs(data_base_dict['imposters'])

"""create_XY()

הפונקציה יוצרת את מערכי הנתונים איקס ו ווי עבור המודל
איקס כולל את כל הווקטורים של המילים שנלקחו משני אימפוסטרים
ו- ווי הוא מערך התווית שמציין לאיזה מחבר שייך כל ווקטור
"""

# Create input data (X) and corresponding labels (Y) for a binary classification task, using two sets of input data.
# The function balances the two input datasets by duplicating the smaller dataset and then shuffling the final combined dataset.
# Parameters:
  # imp_1 - list, First set of input data representing one class.
  # imp_2 - list, Second set of input data representing the other class.
  # data_batch_size - int, The size of each data batch used in training the model.
  # emb_dim - int, The dimensions of the word embeddings used in the input data.
# Returns:
  # X - ndarray, Combined input data from both `imp_1` and `imp_2`, structured as a single array.
  # Y - ndarray, Labels corresponding to the combined input data (X), with '1' for data from `imp_1` and '2' for data from `imp_2`.
def create_XY(imp_1, imp_2, data_batch_size, emb_dim):
  combined_imposters = [imp_1, imp_2]

  # Calculate the lengths of the two datasets together
  combined_len = [len(combined_imposters[0]), len(combined_imposters[1])]
  # # Identifies the largest and smallest dataset
  if len(imp_1) > len(imp_2):
    max_imposter = imp_1
    min_imposter = imp_2
  else:
    max_imposter = imp_2
    min_imposter = imp_1
  max_imposter_index = combined_len.index(max(combined_len))
  min_imposter_index = combined_len.index(min(combined_len))
  div = combined_len[max_imposter_index] // combined_len[min_imposter_index]
  mod = combined_len[max_imposter_index] % combined_len[min_imposter_index]
  print(f"div: {div}")
  print(f"mod: {mod}")
  min_imposter_padded = []
  # # duplicates entries in the smaller dataset until it matches the length of the larger one.
  for i in range(div):
    min_imposter_padded.extend(min_imposter)

  min_imposter_padded.extend(min_imposter[:mod])

  # # Update the combined_imposters with the min_imposter_padded
  print(f"len(min_imposter_padded): {len(min_imposter_padded)}, len(max_imposter): {len(max_imposter)},")
  combined_imposters_padded = [min_imposter_padded, max_imposter]
  combined_len_padded = [len(combined_imposters_padded[0]), len(combined_imposters_padded[1])]

  # # Assigns labels ('1' for `combined_imposters` and '2' for `imp_2`), and shuffles them together.
  X = [item for sublist in combined_imposters_padded for item in sublist]
  print(f"len(X): {len(combined_imposters_padded)}")
  Y = [0] * len(combined_imposters_padded[0]) + [1] * len(combined_imposters_padded[1])

  # Define the target shape for each item
  target_shape = (data_batch_size, emb_dim)

  X = np.array(X)
  Y = np.array(Y)

  # Return the input data (X), labels (Y)
  return X, Y

class CheckpointCleanup(Callback):
  def __init__(self, checkpoint_path, patience):
      super().__init__()
      self.checkpoint_path = checkpoint_path
      self.patience = patience
      self.checkpoint_list = []

  def on_epoch_end(self, epoch, logs=None):
      # Get list of checkpoint files
      checkpoints = glob.glob(os.path.join(self.checkpoint_path, 'model_*.keras'))
      checkpoints.sort(key=os.path.getmtime)  # Sort by modification time

      # Remove older checkpoints if we have more than patience
      while len(checkpoints) > self.patience:
          oldest_checkpoint = checkpoints[0]
          try:
              os.remove(oldest_checkpoint)
              checkpoints.pop(0)
          except Exception as e:
              print(f"Error removing checkpoint: {e}")

"""cnn_model_1()"""

# Build, compile, and train a CNN.
# Parameters:
  # X - ndarray, Input data matrix of shape (num_samples, num_timesteps, embedding_dim), where each sample represents text chunks and their corresponding word embeddings.
  # Y - ndarray, Labels for the classification task, represented as binary values for binary classification.
  # emb_dim - int, The dimensions of the word embeddings used in the input data.
  # data_batch_size - int, The size of each data batch used in training the model.
  # kernel_size - list of int, A list of kernel sizes for each 1D convolutional layer. Each size controls the receptive field of the corresponding convolutional layer.
  # nb_filter - int, The number of filters to apply in each convolutional layer.
  # pool_size - int, The size of the pooling window for the max-pooling layers.
  # dense_outputs - int, optional, The number of neurons in the dense layer. Default is 256.
  # cat_output - int, optional, The number of output classes for the final classification. Default is 10.
  # learning_rate - float, optional, The learning rate for model optimization. Default is 0.001.
  # momentum - float, optional, Momentum for the optimizer to accelerate convergence. Default is 0.9.
  # decay - float, optional, Decay applied to the learning rate to reduce it during training. Default is 0.
  # nb_epoch - int, optional, The number of epochs to train the model. Default is 12.
  # test_size - float, optional, The proportion of the dataset to include in the validation split. Default is 0.33.
  # usePreL - int, optional, Placeholder parameter for enabling preloaded models or layers. Default is 0.
  # mod_name - str, optional, Name of the model for saving plots and checkpoints. Default is 'model_eng'.
  # DropoutP - float, optional, Dropout rate for regularization in the dense layer. Default is 0.25.
# Returns:
  # model - Keras Sequential model, The trained RCNNA model, ready for predictions or further training.
  # history - keras.callbacks.History, A Keras History object that contains the training, validation loss and accuracy for each epoch.

def cnn_model_1(model, X, Y, emb_dim, data_batch_size, kernel_size, nb_filter, pool_size, dense_outputs, cat_output, learning_rate, momentum, decay, nb_epoch, test_size, usePreL, mod_name, DropoutP, kernel_regularizer):
    # Assign training data
    X_train, Y_train = X, Y


    if model == None:
      # Initialize a Sequential model
      model = Sequential()

      print(f"Adding first Conv1D layer with filters={int(nb_filter * 2)}, kernel_size={kernel_size[0]}")
      # First convolutional block with increased filters
      model.add(Conv1D(
          filters=int(nb_filter * 2),
          kernel_size=kernel_size[0],
          padding='same',
          activation='relu',
          input_shape=(300, 1)
      ))
      model.add(BatchNormalization())
      model.add(MaxPooling1D(pool_size=pool_size))
      model.add(Dropout(0.3))
      # Second convolutional block
      model.add(Conv1D(
          filters=int(nb_filter * 1.5),
          kernel_size=kernel_size[1],
          padding='same',
          activation='relu'
      ))
      model.add(BatchNormalization())
      model.add(MaxPooling1D(pool_size=pool_size))
      model.add(Dropout(0.3))

      # Flatten the convolution output
      model.add(Flatten())
      # Modified dense layers with different activation functions
      model.add(Dense(800, activation='relu', kernel_regularizer=l2(kernel_regularizer)))
      model.add(BatchNormalization())
      model.add(Dropout(DropoutP))

      model.add(Dense(dense_outputs, activation='relu', kernel_regularizer=l2(kernel_regularizer)))
      model.add(BatchNormalization())
      model.add(Dropout(DropoutP))

      # Final output layer
      model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(kernel_regularizer)))

    # Using legacy optimizer with proper learning rate schedule
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=learning_rate,
        decay_steps=1000,
        decay_rate=0.9
    )
    optimizer = Adam(learning_rate=lr_schedule)

      # Compile the model
      # model.compile(
      #     loss='binary_crossentropy',
      #     optimizer=optimizer,
      #     metrics=['accuracy']
      # )

    # Early stopping with modified parameters
    patience=5

    es = EarlyStopping(
        monitor='val_loss',
        mode='min',
        patience=patience,
        restore_best_weights=True,
        min_delta=0.001
    )
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    mc = tf.keras.callbacks.ModelCheckpoint(
        f"{colab_output_path}best_model{timestamp}.keras",
        monitor='val_loss',
        mode='min',
        save_best_only=True
    )

    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)

    Y_train = np.array(Y_train, dtype='float32').reshape(-1, 1)

    cleanup_callback = CheckpointCleanup(checkpoint_path, patience)
    # Add them to your callbacks list
    callbacks = [es, mc, cleanup_callback]

    # Modify the compile section to add more metrics for monitoring
    model.compile(
        loss='binary_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy', 'AUC']
    )

    history = model.fit(
        X_train,
        Y_train,
        validation_split=test_size,
        epochs=nb_epoch,
        callbacks=callbacks,
        batch_size=32,
        verbose=1
    )

    # Clean up memory
    del X, Y
    gc.collect()

    return model, history

"""cnn_model_2()"""

# Build, compile, and train a CNN.
# Parameters:
  # X - ndarray, Input data matrix of shape (num_samples, num_timesteps, embedding_dim), where each sample represents text chunks and their corresponding word embeddings.
  # Y - ndarray, Labels for the classification task, represented as binary values for binary classification.
  # emb_dim - int, The dimensions of the word embeddings used in the input data.
  # data_batch_size - int, The size of each data batch used in training the model.
  # kernel_size - list of int, A list of kernel sizes for each 1D convolutional layer. Each size controls the receptive field of the corresponding convolutional layer.
  # nb_filter - int, The number of filters to apply in each convolutional layer.
  # pool_size - int, The size of the pooling window for the max-pooling layers.
  # dense_outputs - int, optional, The number of neurons in the dense layer. Default is 256.
  # cat_output - int, optional, The number of output classes for the final classification. Default is 10.
  # learning_rate - float, optional, The learning rate for model optimization. Default is 0.001.
  # momentum - float, optional, Momentum for the optimizer to accelerate convergence. Default is 0.9.
  # decay - float, optional, Decay applied to the learning rate to reduce it during training. Default is 0.
  # nb_epoch - int, optional, The number of epochs to train the model. Default is 12.
  # test_size - float, optional, The proportion of the dataset to include in the validation split. Default is 0.33.
  # usePreL - int, optional, Placeholder parameter for enabling preloaded models or layers. Default is 0.
  # mod_name - str, optional, Name of the model for saving plots and checkpoints. Default is 'model_eng'.
  # DropoutP - float, optional, Dropout rate for regularization in the dense layer. Default is 0.25.
# Returns:
  # model - Keras Sequential model, The trained RCNNA model, ready for predictions or further training.
  # history - keras.callbacks.History, A Keras History object that contains the training, validation loss and accuracy for each epoch.
def cnn_model_2(model, X, Y, emb_dim, data_batch_size, kernel_size, nb_filter, pool_size, dense_outputs, cat_output, learning_rate, momentum, decay, nb_epoch, test_size, usePreL, mod_name, DropoutP, kernel_regularizer):
    # Assign training data
    X_train, Y_train = X, Y

    # Create checkpoint directory
    checkpoint_path = os.path.join(colab_output_path, 'checkpoints')
    os.makedirs(checkpoint_path, exist_ok=True)

    if model == None:
      # Initialize a Sequential model
      model = Sequential()

      # First convolutional layer
      model.add(Conv1D(
          filters=nb_filter,
          kernel_size=kernel_size[0],
          padding='valid',
          activation='relu',
          input_shape=(300, 1)
      ))
      model.add(BatchNormalization())
      model.add(Dropout(DropoutP))

      # Second convolutional layer
      model.add(Conv1D(
          filters=nb_filter,
          kernel_size=kernel_size[1],
          padding='valid',
          activation='relu'
      ))
      model.add(BatchNormalization())
      model.add(MaxPooling1D(pool_size=pool_size))
      model.add(Dropout(DropoutP))

      # Flatten the convolution output
      model.add(Flatten())

      # Output layer - Changed to single output with sigmoid for binary classification
      model.add(Dense(
          1,
          activation='sigmoid',
          kernel_regularizer=l2(kernel_regularizer)
      ))
      model.add(BatchNormalization())

      # Learning rate schedule
      lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
          initial_learning_rate=learning_rate,
          decay_steps=1000,
          decay_rate=0.9
      )
      optimizer = Adam(learning_rate=lr_schedule)

    patience = 5
    # Early stopping
    es = EarlyStopping(
        monitor='val_loss',
        mode='min',
        patience=patience,
        restore_best_weights=True,
        min_delta=0.001
    )

    # Model checkpoint
    mc = tf.keras.callbacks.ModelCheckpoint(
        filepath=os.path.join(checkpoint_path, 'model_{epoch:02d}_valloss_{val_loss:.4f}.keras'),
        monitor='val_loss',
        mode='min',
        save_best_only=True,
        save_weights_only=False,
    )

    cleanup_callback = CheckpointCleanup(checkpoint_path, patience)

    # Add callbacks
    callbacks = [es, mc, cleanup_callback]

    # Compile
    model.compile(
        loss='binary_crossentropy',
        optimizer=optimizer,
        metrics=['accuracy', 'AUC']
    )

    if iteration_num == 1:
        plot_model(model,
                  to_file=colab_output_path + str(iteration_num) + 'model_plot.png',
                  show_shapes=True,
                  show_layer_names=True)

    print('Fit model...')

    # Ensure X and Y are properly shaped
    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    Y_train = np.array(Y_train, dtype='float32').reshape(-1, 1)

    # Train the model
    history = model.fit(
        X_train,
        Y_train,
        validation_split=test_size,
        epochs=nb_epoch,
        callbacks=callbacks,
        batch_size=32,
        verbose=1
    )

    # Clean up memory
    del X, Y
    gc.collect()

    return model, history

"""
classify_text

זו אחת הפונקציות המרכזיות בקוד. היא מבצעת את עיבוד הטקסטים והסיווג שלהם על בסיס הפרמטרים שהוזנו. הפונקציה מבצעת:

עיבוד מקדים לטקסטים.
הטמעת מילים וורד אמבדינג באמצעות  אלגוריתם וורד טו ווק
אימון רשת עצבית מבוסס סי אן אן כדי לסווג את הטקסט
יציאת מטריצת מרחקים בין הטקסטים השונים"""

def calculate_moving_averages(data, moving_average_window_size):
    data = np.asarray(data).flatten()
    np_mavg = np.convolve(data, np.ones(moving_average_window_size)/moving_average_window_size, mode='valid')
    return np_mavg

def calculate_dtw_simliraity(moving_average_a, moving_average_b):
  print(f"moving_average_a: {moving_average_a}, moving_average_b: {moving_average_b}")
  distance, paths = dtw.warping_paths_fast(moving_average_a, moving_average_b, use_c=False)
  best_path = dtw.best_path(paths)
  similarity_score = distance / len(best_path)
  return similarity_score

def postprocessing(model, imposters_pair_moving_average, moving_average_window_size, imposter_a, imposter_b):
  covid_variant_dtw_simliarity_list = []
  print(f"imposter_a['name']: {imposter_a['name']}, imposter_b['name']: {imposter_b['name']}")
  for covid_variant in tqdm(data_base_dict['covid19'], desc="Processing covid variants for pair"):
      print(f"covid_variant['name']: {covid_variant['name']}")
      imposters_pair_covid_dict = {}
      imposters_pair_covid_dict['y_pred'] = model.predict(covid_variant['X'])
      covid_variant_moving_average = calculate_moving_averages(imposters_pair_covid_dict['y_pred'], moving_average_window_size)
      imposters_pair_covid_dict['name'] = covid_variant['name']
      imposters_pair_covid_dict['dtw_similarity_score'] = calculate_dtw_simliraity(imposters_pair_moving_average, covid_variant_moving_average)
      covid_variant_dtw_simliarity_list.append(imposters_pair_covid_dict)
  return covid_variant_dtw_simliarity_list

# Classify text using a combination of preprocessing, word embedding, and CNN. It also computes a distance matrix using DTW and applies k-medoids clustering on the results. This function outputs classification results, plots model accuracy and loss, and saves key data structures.
# Parameters:
  # params_obj - dict, A dictionary of parameters used for the classification process. It includes:
    # dimensional - str:, Dimensions of the word embeddings.
    # chunk - str, Size of text chunks for preprocessing.
    # embedding_algorithm - str, Embedding algorithm to use, e.g., Word2Vec.
    # imposter_a - str, Name of the first imposter for the current iteration.
    # imposter_b - str, Name of the second imposter for the current iteration.
    #profile
def classify_text(params_obj, cnn_model=None):
      # Global parameters:
      # iteration_num - int, The current iteration number.
      # sum_dicForest - dict, Dictionary containing classification results from the Isolation Forest algorithm.
      # sum_dic_summ - dict, Dictionary summarizing classification results.
      # k_medoids - list, Data structure storing results from k-medoids clustering.
      # num_lstm - int, Number of LSTM networks used in the process.
      # k_medoidsL - list, Labels from k-medoids clustering.
      # dist_mat2 - ndarray, Matrix storing distances between imposters after processing.
      # dist_mat - ndarray, Distance matrix used for clustering and presentation.
      # dict_test2 - dict, Dictionary storing test results for each file after processing with the model.
      # t_model - object, TensorFlow or Keras model used during text classification.

    global iteration_num, sum_dicForest, sum_dic_summ, k_medoids, num_lstm, k_medoidsL, dist_mat2, dist_mat, dict_test2, t_model

    # Word embedding dimensionality
    emb_dim = int(params_obj['dimensional'])
    data_batch_size = 16

    # Model and training parameters
    iter_idx = iteration_num
    kernel_size = [2, 3, 6]

    nb_filter = 100
    pool_size = 1
    dense_outputs = 256
    cat_output = 2
    learning_rate = 0.001
    momentum = 0.9
    decay = 1
    if is_debug_2:
      nb_epoch = 1
    else:
      nb_epoch = 5
    test_size = 0.20
    usePreL = 0
    mod_name = 'model_eng'
    DropoutP = 0.5
    kernel_regularizer = 0.001
    X, y = create_XY(params_obj['imposter_a']['X'], params_obj['imposter_b']['X'], data_batch_size, emb_dim)

    # Train the Convolutional Neural Network (CNN) model 1
    print("Start CNN model")
    cnn_model, history = cnn_model_1(cnn_model, X, y, emb_dim, data_batch_size, kernel_size, nb_filter, pool_size, dense_outputs, cat_output, learning_rate, momentum, decay, nb_epoch, test_size, usePreL, mod_name, DropoutP, kernel_regularizer)

    # # Train the Convolutional Neural Network (CNN) model 2
    # print("Start CNN model")
    # cnn_model, history = cnn_model_2(X, Y, emb_dim, data_batch_size, kernel_size, nb_filter, pool_size, dense_outputs, cat_output, learning_rate, momentum, decay, nb_epoch, test_size, usePreL, mod_name, DropoutP, kernel_regularizer)

    # # Save the trained model
    # cnn_model.save(colab_output_path + 'best_model' + str(iteration_num) + '.keras')
    # print("Finish CNN model")

    # Plot training accuracy and validation accuracy
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc = 'upper left')
    plt.show()

    # Plot training loss and validation loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc = 'upper left')
    plt.show()
    print(f"X: {X}")
    y_pred = cnn_model.predict(X)
    print(f"y_pred: {y_pred}")
    imposters_pair_moving_average = calculate_moving_averages(y_pred, params_obj['moving_average_window_size'])
    covid_variant_dtw_simliarity_list = postprocessing(cnn_model, imposters_pair_moving_average, params_obj['moving_average_window_size'], params_obj['imposter_a'], params_obj['imposter_b'])
    return cnn_model, y_pred, covid_variant_dtw_simliarity_list

if is_debug:
  data_file_path = f"{colab_output_path}data_20241108_160307.pkl"
  with open(data_file_path, 'rb') as f:
      imposters_pair_pairs_dict_list = pickle.load(f)

"""presentationRandomForest()

הפונקציה הזו נועדה לבצע זיהוי של חריגות ולהציג את התוצאות בעזרת אלגוריתם

 IsolationForest.

היא מקבלת מטריצת מרחקים

 (dist_mat)

ומבצעת ניתוח על הנתונים, תוך דגש על זיהוי אנומליות (חריגות) בתוצאות
"""

def binary_encode(y, is_outlier_detection=False):
    if is_outlier_detection:
      mapping = {1: 0, -1: 1}
    else:
      mapping = {'imposters_pairs_pair': 0, 'covid': 1}
    encoded = y.map(mapping)
    return encoded

def detect_outliers(
    data: Union[pd.DataFrame, np.ndarray],
    contamination: float = 0.1,
    n_estimators: int = 100,
    return_scores: bool = False
) -> Union[Tuple[np.ndarray, np.ndarray], np.ndarray]:
    if isinstance(data, pd.DataFrame):
        data = data.to_numpy()

    iso_forest = IsolationForest(
        n_estimators=n_estimators,
        contamination=contamination,
        random_state=random_seed,
        max_samples='auto'
    )

    predictions = iso_forest.fit_predict(data)

    if return_scores:
        scores = iso_forest.score_samples(data)
        return predictions, scores

    return predictions

def analyze_outliers(
    data: Union[pd.DataFrame, np.ndarray],
    predictions: np.ndarray,
    scores: Optional[np.ndarray] = None
) -> dict:
    n_samples = len(predictions)
    n_outliers = (predictions == -1).sum()

    analysis = {
        'total_samples': n_samples,
        'outliers_count': n_outliers,
        'outliers_percentage': (n_outliers / n_samples) * 100,
        'inliers_count': (predictions == 1).sum(),
    }

    if scores is not None:
        analysis.update({
            'min_score': scores.min(),
            'max_score': scores.max(),
            'mean_score': scores.mean(),
            'median_score': np.median(scores)
        })

    return analysis

if is_debug:
  X = df
  y = X.pop('variant_type')

if is_debug:
  X['is_outlier_isolation_forest'] = detect_outliers(X)
  df = pd.concat([X, y], ignore_index=True)
  df_with_outliers_isolation_forest = pd.concat([X['dtw_similarity_score'], y, X['is_outlier_isolation_forest']], axis=1)
  print(df_with_outliers_isolation_forest)

if is_debug:
  analysis = analyze_outliers(X['dtw_similarity_score'], X['is_outlier_isolation_forest'])
  print(analysis)

if is_debug:
  value_counts = df_with_outliers_isolation_forest['variant_type'].value_counts()
  print(value_counts)

def detect_outliers_rf(
    data: pd.DataFrame,
    contamination: float = 0.1,
    random_state: int = 42,
    n_estimators: int = 100,
    return_scores: bool = False,
    threshold: Optional[float] = None
) -> Union[Tuple[np.ndarray, np.ndarray], np.ndarray]:
    """
    Detect outliers using Random Forest regression.
    Modified to handle single-feature cases by using sliding windows.
    """
    if isinstance(data, pd.DataFrame):
        data = data.to_numpy()

    if len(data.shape) == 1:
        data = data.reshape(-1, 1)

    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data)
    n_samples = scaled_data.shape[0]

    if scaled_data.shape[1] == 1:
        window_size = 5
        X = np.zeros((n_samples - window_size, window_size))
        y = np.zeros(n_samples - window_size)

        for i in range(len(X)):
            X[i] = scaled_data[i:i + window_size, 0]
            y[i] = scaled_data[i + window_size, 0]

        rf = RandomForestRegressor(
            n_estimators=n_estimators,
            random_state=random_state,
            n_jobs=-1
        )
        rf.fit(X, y)

        predictions = np.zeros(n_samples)
        predictions[:window_size] = scaled_data[:window_size, 0]

        for i in range(len(X)):
            predictions[i + window_size] = rf.predict(X[i].reshape(1, -1))

        reconstruction_errors = np.square(scaled_data[:, 0] - predictions)

    if threshold is None:
        threshold = np.percentile(
            reconstruction_errors,
            100 * (1 - contamination)
        )

    predictions = np.where(reconstruction_errors > threshold, -1, 1)

    if return_scores:
        return predictions, reconstruction_errors

    return predictions

def analyze_rf_outliers(
    data: Union[pd.DataFrame, np.ndarray],
    predictions: np.ndarray,
    scores: Optional[np.ndarray] = None,
    feature_names: Optional[list] = None
) -> dict:
    n_samples = len(predictions)
    n_outliers = (predictions == -1).sum()
    analysis = {
        'total_samples': n_samples,
        'outliers_count': n_outliers,
        'outliers_percentage': (n_outliers / n_samples) * 100,
        'inliers_count': (predictions == 1).sum()
    }

    if scores is not None:
        analysis.update({
            'reconstruction_error_stats': {
                'min': scores.min(),
                'max': scores.max(),
                'mean': scores.mean(),
                'median': np.median(scores),
                'std': scores.std()
            }
        })

    return analysis

if is_debug:
  X['is_outlier_random_forest'] = detect_outliers_rf(X)
  df_rf = pd.concat([X, y], ignore_index=True)
  df_with_outliers_random_forest = pd.concat([X['dtw_similarity_score'], y, X['is_outlier_random_forest']], axis=1)
  print(df_with_outliers_random_forest)

if is_debug:
  print(df_with_outliers_random_forest)

if is_debug:
  analysis = analyze_rf_outliers(X['dtw_similarity_score'], X['is_outlier_random_forest'])
  print(analysis)

if is_debug:
  value_counts = df_with_outliers_random_forest['variant_type'].value_counts()
  print(value_counts)

def perform_kmedoids_clustering(
    df: pd.DataFrame,
    n_clusters: int = 2,
    random_state: int = 42
) -> Tuple[pd.DataFrame, Dict, np.ndarray]:

    X = df['dtw_similarity_score'].values.reshape(-1, 1)

    kmedoids = KMedoids(
        n_clusters=n_clusters,
        random_state=random_state,
        metric='manhattan'
    )

    cluster_labels = kmedoids.fit_predict(X)

    result_df = df.copy()
    result_df['cluster'] = cluster_labels

    medoids = kmedoids.cluster_centers_.flatten()
    medoid_indices = kmedoids.medoid_indices_

    cluster_info = {}
    for i in range(n_clusters):
        cluster_data = result_df[result_df['cluster'] == i]['dtw_similarity_score']
        cluster_info[f'Cluster {i}'] = {
            'size': len(cluster_data),
            'medoid': medoids[i],
            'mean': cluster_data.mean(),
            'std': cluster_data.std(),
            'min': cluster_data.min(),
            'max': cluster_data.max()
        }

    plt.figure(figsize=(10, 6))

    for i in range(n_clusters):
        cluster_points = result_df[result_df['cluster'] == i]['dtw_similarity_score']
        plt.scatter(cluster_points, [0.1] * len(cluster_points),
                   alpha=0.5, label=f'Cluster {i}')

    plt.scatter(medoids, [0.1] * len(medoids),
               color='red', marker='*', s=200,
               label='Medoids')

    plt.yticks([])
    plt.xlabel('Value')
    plt.title('K-Medoids Clustering Results')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    return result_df, cluster_info, medoid_indices

if is_debug:
  perform_kmedoids_clustering(X)

def perform_kmeans_clustering(
    df: pd.DataFrame,
    n_clusters: int = 2,
) -> Tuple[pd.DataFrame, Dict, np.ndarray]:

    # Reshape data for KMeans
    X = df['dtw_similarity_score'].values.reshape(-1, 1)

    # Initialize and fit KMeans
    kmeans = KMeans(
        n_clusters=2,
        random_state=random_seed,
        n_init='auto'
    )

    # Fit and predict
    cluster_labels = kmeans.fit_predict(X)

    # Add cluster labels to DataFrame
    result_df = df.copy()
    result_df['cluster'] = cluster_labels

    # Get centroids
    centroids = kmeans.cluster_centers_.flatten()

    # Calculate cluster statistics
    cluster_info = {}
    for i in range(n_clusters):
        cluster_data = result_df[result_df['cluster'] == i]['dtw_similarity_score']
        cluster_info[f'Cluster {i}'] = {
            'size': len(cluster_data),
            'centroid': centroids[i],
            'mean': cluster_data.mean(),
            'std': cluster_data.std(),
            'min': cluster_data.min(),
            'max': cluster_data.max(),
            'inertia': np.sum((cluster_data - centroids[i])**2)  # Within-cluster sum of squares
        }

    # Add total inertia
    cluster_info['total_inertia'] = kmeans.inertia_

    # Visualize the clustering
    plt.figure(figsize=(10, 6))

    # Plot original points
    for i in range(n_clusters):
        cluster_points = result_df[result_df['cluster'] == i]['dtw_similarity_score']
        plt.scatter(cluster_points, [0.1] * len(cluster_points),
                   alpha=0.5, label=f'Cluster {i}')

    # Plot centroids
    plt.scatter(centroids, [0.1] * len(centroids),
               color='red', marker='x', s=200,
               label='Centroids')

    plt.yticks([])
    plt.xlabel('Value')
    plt.title('K-Means Clustering Results')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    return result_df, cluster_info, centroids

if is_debug:
  perform_kmeans_clustering(X)

"""## Run Main"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# data_dict = main(is_train_word2vec, data_file_path, iteration_num, data_base_dict, moving_average_window_size, df_csv_path)

